<!doctype html>
<html lang="en">
	<head>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

		<title>Neural Scene Representations</title>

		<link rel="stylesheet" href="dist/reset.css">
		<link rel="stylesheet" href="dist/reveal.css">
		<link rel="stylesheet" href="dist/theme/kth.css">

		<!-- Theme used for syntax highlighted code -->
		<link rel="stylesheet" href="plugin/highlight/monokai.css">
	</head>
	<body>
		<div class="reveal">
			<div class="slides">

				<section data-auto-animate>
					<h1>Overview</h1>
					<h2></h2>
					<ul>
						<li>Introduction: neural fields, NeRFs, etc.</li> <!-- 10 minutes -->
						<br />
						<li>Neural scene representations for SLAM</li> <!-- 10 minutes -->
						<br />
						<li>Efficient loop closure in neural SLAM</li>  <!-- 10 minutes -->
						<br />
						<li>Open research problems</li> <!-- 10 minutes -->
					</ul>
				</section>
				<section data-auto-animate>
					<h4>Introduction</h4>
					<h1>Shape and Scene Representations</h1>
					<h2></h2>
					<ul>
						<li>Various shape / scene representations exist</li>
						<li class="fragment" data-fragment-index="2">Surface representations
							<ul>
								<li class="fragment" data-fragment-index="2">Store surface position and quantities on surface</li>
								<li class="fragment" data-fragment-index="2">Examples
									<ul>
										<li class="fragment" data-fragment-index="2">Meshes</li>
										<li class="fragment" data-fragment-index="3">Point clouds</li>
									</ul>
								</li>
							</ul>
						</li>
						<li class="fragment" data-fragment-index="4">Volumetric representations
							<ul>
								<li class="fragment" data-fragment-index="4">Store quantities in volume</li>
								<li class="fragment" data-fragment-index="4">Examples
									<ul>
										<li class="fragment" data-fragment-index="4">Voxel grid (dense, octrees, voxel hashing)</li>
										<li class="fragment" data-fragment-index="5">Neural fields</li>
										<li class="fragment" data-fragment-index="6">Set of Gaussians</li>
									</ul>
								</li>
							</ul>
						</li>
					</ul>
					<img class="fragment fade-in-then-out" data-fragment-index="2" style="position: absolute; top: 400px; left: 1300px" src="assets/mesh.png" width="400px">
					<img class="fragment fade-in-then-out" data-fragment-index="3" style="position: absolute; top: 400px; left: 1300px" src="assets/pointcloud.png" width="400px">
					<img class="fragment fade-in-then-out" data-fragment-index="4" style="position: absolute; top: 400px; left: 1300px" src="assets/octree.jpg" width="400px">
					<img class="fragment fade-in-then-out" data-fragment-index="5" style="position: absolute; top: 400px; left: 1100px" src="assets/deepsdf.png" width="600px">
					<!-- mesh Robust Visibility Surface Determination in Object Space via PlÃ¼cker Coordinates -->
					<!-- pointcloud source Analyzing the Squared Distance-to-Measure Gradient Flow System with k-Order Voronoi Diagrams -->
					<!--Octree source https://developer.download.nvidia.com/books/gpugems2/37_octree_03.jpg -->
				</section>
				<section data-auto-animate>
					<h4>Introduction</h4>
					<h1>Neural Fields</h1>
					<h2></h2>
					<ul>
						<li>Core idea: represent 3D shape by a simple neural network
							\[
							\begin{aligned}
							f: \mathbb{R}^{3} &\to \mathbb{R}^3\times\mathbb{R}\\
							\boldsymbol{x}&\mapsto (\boldsymbol{c}, \rho)
							\end{aligned}
							\]
						</li>
						<li class="fragment" data-fragment-index="2">Map 3D point $\boldsymbol{x}$ to quantity $\rho$ (e.g., SDF, occupancy, color, etc.)</li>
						<li class="fragment" data-fragment-index="3">Properties
							<ul>
								<li>Continuous</li>
								<li>Differentiable</li>
								<li>Adaptive $\to$ potentially memory efficient</li>
							</ul>
						</li>
					</ul>
					<div style="position: absolute; top: 200px; left: 1300px">
						<img src="assets/staircase.gif" width="500px">
						<br />
						<img src="assets/staircase_sdf.gif" width="500px">
					</div>
					<!-- <img style="position: absolute; top: 150px; right: 20px;" src="assets/staircase.gif" width="25%"> -->
				</section>
				<section data-auto-animate>
					<h4>Introduction</h4>
					<h1>Neural Scene Representations</h1>
					<h2>Early Works</h2>
					<ul>
						<li>Trained using direct 3D supervision (i.e., from mesh collections)</li>
						<li>DeepSDF, occupancy network, ...</li>
						<li>Demonstrate shape interpolation by adding additional latent input</li>
						\[
						\begin{aligned}
						f: \mathbb{R}^{3} \times \mathbb{R}^N &\to \mathbb{R}\\
						(\boldsymbol{x}, \boldsymbol{z})&\mapsto \sigma
						\end{aligned}
						\]
					</ul>
					<br />
					<img style="margin-left:200px" src="assets/deepsdf.png" width="25%">
					<video data-autoplay loop style="margin-left: 200px" width="40%" data-src="assets/deepsdf_interpolation.webm" />
						<!-- video source https://www.youtube.com/watch?v=C_XNdGGs6qM -->
				</section>
				<section data-auto-animate>
					<h4>Introduction</h4>
					<h1>Neural Scene Representations</h1>
					<h2>Neural Radiance Field (NeRF)</h2>
					<ul>
						<li>Train neural field using indirect 2D supervision</li>
						<li>Camera poses fixed (e.g., from structure-from-motion)</li>
						<li>Add 2D view direction as additional input</li>
						\[
						\begin{aligned}
						f: \mathbb{R}^{3} \times \mathbb{R}^2 &\to \mathbb{R}^3\times\mathbb{R}\\
						(\boldsymbol{x}, \boldsymbol{d})&\mapsto (\boldsymbol{c}, \sigma)
						\end{aligned}
						\]
					</ul>
					<br />
					<br />
					<img style="margin-left:200px" src="assets/nerf_overview.png" width="75%">
				</section>
				<section data-auto-animate>
					<section data-auto-animate>
						<h4>Introduction</h4>
						<h1>Neural Scene Representations</h1>
						<h2>Neural Radiance Field (NeRF)</h2>
						<ul>
							<li>NeRF Examples<br />
								<video loop style="" height="400px">
									<source data-autoplay data-src="assets/nerf_1.mp4">
								</video>
								<video loop style="" height="400px">
									<source data-autoplay data-src="assets/nerf_2.mp4">
								</video>
								<video loop style="" height="400px">
									<source data-autoplay data-src="assets/nerf_3.mp4">
								</video>
								<span class="fragment fade-in">
									<video data-autoplay loop height="400px" data-src="assets/nerf_depth.mp4" />
								</span>
							</li>
						</ul>
					</section>
					<section data-auto-animate>
						<h4>Introduction</h4>
						<h1>Neural Scene Representations</h1>
						<h2>Neural Radiance Field (NeRF)</h2>
						<ul>
							<li>NeRF Examples<br />
								<video loop style="" height="250px">
									<source data-autoplay data-src="assets/nerf_1.mp4">
								</video>
								<video loop style="" height="250px">
									<source data-autoplay data-src="assets/nerf_2.mp4">
								</video>
								<video loop style="" height="250px">
									<source data-autoplay data-src="assets/nerf_3.mp4">
								</video>
								<span>
									<video data-autoplay loop height="250px" data-src="assets/nerf_depth.mp4" />
								</span>
							</li>
							<li>Limitations
								<ul>
									<li>Mainly limited to small forward-facing scenes</li>
									<li>Isolevel for surfaces not well-defined</li>
									<li>Optimization time (~hours)</li>
									<li>Rendering time (~seconds for single 1080p frame)</li>
								</ul>
							</li>
						</ul>
					</section>
				</section>
				<section data-auto-animate>
					<section data-auto-animate>
						<h4>Introduction</h4>
						<h1>Neural Scene Representations</h1>
						<h2>Follow-up Works</h2>
						<ul>
							<li>Surfaces: NeuS <br />
								<img src="assets/neus.png" height="600px">
							</li>
						</ul>
					</section>
					<section data-auto-animate>
						<h4>Introduction</h4>
						<h1>Neural Scene Representations</h1>
						<h2>Follow-up Works</h2>
						<ul>
							<li>Surfaces: NeuS <br />
								<img src="assets/neus.png" height="200px">
							</li>
							<li>Speed: Instant Neural Graphics Primitives<br />
								<video loop data-autoplay height="500px" data-src="assets/ingp.mp4" />
							</li>
						</ul>
					</section>
					<section data-auto-animate>
						<h4>Introduction</h4>
						<h1>Neural Scene Representations</h1>
						<h2>Follow-up Works</h2>
						<ul>
							<li>Surfaces: NeuS <br />
								<img src="assets/neus.png" height="200px">
							</li>
							<li>Speed: Instant Neural Graphics Primitives<br />
								<video loop data-autoplay height="200px" data-src="assets/ingp.mp4" />
							</li>
							<li>Quality: Zip-NeRF<br />
							</li>
							<video style="left: 800px; bottom: 570px;position: relative;" loop data-autoplay height="800px" data-src="assets/zipnerf.mp4" />
						</ul>
					</section>
					<section data-auto-animate>
						<h4>Introduction</h4>
						<h1>Neural Scene Representations</h1>
						<h2>Follow-up Works</h2>
						<ul>
							<li>Surfaces: NeuS <br />
								<img src="assets/neus.png" height="200px">
							</li>
							<li>Speed: Instant Neural Graphics Primitives<br />
								<video loop data-autoplay height="200px" data-src="assets/ingp.mp4" />
							</li>
							<li>Quality: Zip-NeRF<br />
							</li>
							<video style="left: 0; top: 0;position: relative;" loop data-autoplay height="200px" data-src="assets/zipnerf.mp4" />
						</ul>
					</section>
				</section>
				<section data-auto-animate>
					<h4>Neural Scene Representations for SLAM</h4>
					<h1>Dense Visual SLAM</h1>
					<h2></h2>
					<ul>
						<li>Visual Simultaneous Localization and Mapping (SLAM)
							<ul>
								<li>Estimate camera trajectory from moving camera</li>
								<li>Sparse keypoint-based maps</li>
							</ul>
						</li>
						<li  class="fragment">Dense visual SLAM
							<ul>
								<li>Estimate dense 3D scene representation during localization (e.g., mesh)</li>
								<li>RGB-D data</li>
								<li>Volumetric scene representation
									<ul>
										<li>Classic approaches use voxel grids to represent 3D scene</li>
									</ul>
								</li>
							</ul>
							<video data-autoplay loop style="left: 0px; top: 50px; position: relative;" height="300px" data-src="assets/slam_input.mp4"></video>
							<video data-autoplay loop style="left: 100px; top: 50px; position: relative;" height="400px" data-src="assets/slam_output.mp4"></video>
						</li>
					</ul>
				</section>
				<section data-auto-animate>
					<h4>Neural Scene Representations for SLAM</h4>
					<h1>iMAP</h1>
					<h2></h2>
					<ul>
						<li>First work demonstrating SLAM using neural field as scene representation</li>
						<li>Alternates between frame-to-map alignment and map refinement</li>
						<li>Compared to NeRF
							<ul>
								<li>Removed view-direction dependency</li>
								<li>Smaller neural network</li>
								<li>Depth supervision</li>
							</ul>
						</li>
						<img  class="fragment" data-fragment-index="2" src="assets/imap.gif" height="400px">

						<li  class="fragment" data-fragment-index="3" >Limitation: continuous reoptimization of previous areas required to prevent forgetting</li>
					</ul>
				</section>
				<section data-auto-animate>
					<section data-auto-animate>
						<h4>Neural Scene Representations for SLAM</h4>
						<h1>NICE-SLAM</h1>
						<h2></h2>
						<ul>
							<li>Replaces single MLP by combination of feature grid and frozen, pretrained decoder
								<br />$\to$ No more forgetting, better scalability
							</li>
						</ul>
						<img src="assets/niceslam.png" height="600px">
					</section>
					<section data-auto-animate>
						<h4>Neural Scene Representations for SLAM</h4>
						<h1>NICE-SLAM</h1>
						<h2></h2>
						<ul>
							<li>Replaces single MLP by combination of feature grid and frozen, pretrained decoder
								<br />$\to$ No more forgetting, better scalability
							</li>
						</ul>
						<video data-autoplay loop style="left: 0px; top: 0;position: relative;" height="850px" data-src="assets/niceslam.mp4" />
					</section>
				</section>
				<section data-auto-animate>
					<section data-auto-animate>
						<h4>Neural Scene Representations for SLAM</h4>
						<h1>Follow-up Works</h1>
						<h2></h2>
						<ul style="width: 100%">
							<li>ESLAM: triplane encoding
								<br />
								<img src="assets/eslam.jpg" height="800px">
							</li>
						</ul>
					</section>
					<section data-auto-animate>
						<h4>Neural Scene Representations for SLAM</h4>
						<h1>Follow-up Works</h1>
						<h2></h2>
						<ul style="width: 100%">
							<li>ESLAM: triplane encoding
								<br />
								<img src="assets/eslam.jpg" height="150px">
							</li>
							<li>Co-SLAM: joint hash / coordinate encoding
								<br />
								<img src="assets/coslam.png" height="550px" />
							</li>
						</ul>
					</section>
					<section data-auto-animate>
						<h4>Neural Scene Representations for SLAM</h4>
						<h1>Follow-up Works</h1>
						<h2></h2>
						<ul style="width: 100%">
							<li>ESLAM: triplane encoding
								<br />
								<img src="assets/eslam.jpg" height="150px">
							</li>
							<li>Co-SLAM: joint hash / coordinate encoding
								<br />
								<img src="assets/coslam.png" height="150px" />
							</li>
							<li>GO-SLAM: loop closure via reoptimization
								<br />
								<img style="position: relative; left: 850px; top: -200px;"
			 src="assets/goslam.png" height="450" />
							</li>
						</ul>
					</section>
					<section data-auto-animate>
						<h4>Neural Scene Representations for SLAM</h4>
						<h1>Follow-up Works</h1>
						<h2></h2>
						<ul style="width: 100%">
							<li>ESLAM: triplane encoding
								<br />
								<img src="assets/eslam.jpg" height="150px">
							</li>
							<li>Co-SLAM: joint hash / coordinate encoding
								<br />
								<img src="assets/coslam.png" height="150px" />
							</li>
							<li>GO-SLAM: loop closure via reoptimization
								<br />
								<img style="position: relative; left: 0px; top: 0px;"
			 src="assets/goslam.png" height="150" />
							</li>
						</ul>
					</section>
				</section>
				<section data-auto-animate>
					<h4>Neural Scene Representations for SLAM</h4>
					<h1>Common Limitation: Monolithic Field</h1>
					<h2></h2>
					<ul>
						<li>All methods use a single monolithic data structure</li>
						<br />
						<li>Expensive to optimize neural fields</li>
						<br />
						<li>Sensor data gets baked into a data structure
							<ul>
								<li>Similar to classic voxel-based data structures</li>
							</ul>
						</li>
						<br />
						<li>Cannot easily deform volumetric scene representation on loop closure
							<ul>
								<li>GO-SLAM has to reintegrate all keyframes that have moved on loop closure<br />
									$\to$ The larger the loop, the more reoptimization is required

								</li>
							</ul>
						</li>
					</ul>
				</section>
				<section>
					<section>
						<h1>Efficient Loop Closure for Neural SLAM</h1>
						<ul>
							<li>Idea: attach lightweight neural fields to pose graph of sparse visual SLAM</li>
							<li>Each field captures scene within a fixed radius $r$</li>
						</ul>
						<img style="position: relative; top: 70px;" src="assets/ngm_anim_0.png" height="700px">
					</section>
					<section data-transition="none">
						<h1>Efficient Loop Closure for Neural SLAM</h1>
						<ul>
							<li>Idea: attach lightweight neural fields to pose graph of sparse visual SLAM</li>
							<li>Each field captures scene within a fixed radius $r$</li>
						</ul>
						<img style="position: relative; top: 70px;" src="assets/ngm_anim_1.png" height="700px">
					</section>
					<section data-transition="none">
						<h1>Efficient Loop Closure for Neural SLAM</h1>
						<ul>
							<li>Idea: attach lightweight neural fields to pose graph of sparse visual SLAM</li>
							<li>Each field captures scene within a fixed radius $r$</li>
						</ul>
						<img style="position: relative; top: 70px;" src="assets/ngm_anim_2.png" height="700px">
					</section>
					<section data-transition="none">
						<h1>Efficient Loop Closure for Neural SLAM</h1>
						<ul>
							<li>Idea: attach lightweight neural fields to pose graph of sparse visual SLAM</li>
							<li>Each field captures scene within a fixed radius $r$</li>
						</ul>
						<img style="position: relative; top: 70px;" src="assets/ngm_anim_3.png" height="700px">
					</section>
					<section data-auto-animate data-transition="none">
						<h1>Efficient Loop Closure for Neural SLAM</h1>
						<ul>
							<li>Idea: attach lightweight neural fields to pose graph of sparse visual SLAM</li>
							<li>Each field captures scene within a fixed radius $r$</li>
						</ul>
						<img style="position: relative; top: 70px;" src="assets/ngm_anim_4.png" height="700px">
					</section>
					<section data-auto-animate>
						<h1>Efficient Loop Closure for Neural SLAM</h1>
						<ul>
							<li>Idea: attach lightweight neural fields to pose graph of sparse visual SLAM</li>
							<li>Each field captures scene within a fixed radius $r$</li>
						</ul>
						<img style="position: relative; top: 70px;" src="assets/ngm_anim_4.png" height="700px">
						<img style="position: relative; top: 70px; left: 300px;" src="assets/ngm_example.png" height="700px">
					</section>
				</section>
				<section data-auto-animate>
					<h4>Efficient Loop Closure for Neural SLAM</h4>
					<h1>Example</h1>
					<video style="left: 0; top: 0;position: relative;" loop data-autoplay height="850px" data-src="assets/ngm_scannet.mp4" />
				</section>
				<section data-auto-animate>
					<h4>Efficient Loop Closure for Neural SLAM</h4>
					<h1>Another Example</h1>
					<video style="left: 0; top: 0;position: relative;" loop data-autoplay height="600px" data-src="assets/ngm_apartment0_cut.mp4" />
				</section>
				<section data-auto-animate>
					<h4>Efficient Loop Closure for Neural SLAM</h4>
					<h1>Comparison to Monolithic Approaches</h1>
					<img style="left: 0; top: 0;position: relative;" src="assets/ngm_comparison.png" height="800px" />
				</section>
				<section data-auto-animate>
					<h4>Efficient Loop Closure for Neural SLAM</h4>
					<h1>Outlook</h1>
					<ul>
						<li>Higher memory usage compared to monolithic neural field methods</li>
						<ul>
							<li>How to reduce memory requirements?</li>
						</ul>
						</li>
						<br />
						<li>Neural field-based map not used for SLAM currently
							<ul>
								<li>How to combine sparse visual SLAM and dense visual SLAM?</li>
							</ul>
						</li>
						<br />
						<li>Currently limited to indoor environments with depth data available
							<ul>
								<li>How to extend to outdoor environments?</li>
							</ul>
						</li>
						<br />
						<li>Neural SLAM in general requires lots of resources
							<ul>
								<li>Can we use meta-learning for few-shot mapping?</li>
							</ul>
						</li>
					</ul>
				</section>
				<section data-auto-animate>
					<section data-auto-animate>
						<h1>Future</h1>
						<h2>Learning vs Optimization</h2>
						<ul>
							<li>Optimization-based approaches inherently limited to what is captured by sensors
								<ul>
									<li>Sufficient for elaborate sensor setups and human operator</li>
									<li>However, far from human-level spatial understanding (infilling / etc.)</li>
								</ul>
							</li>
							<li>Learning approaches might infill from partial observations</li>
							<li class="fragment" data-fragment-index="1">How to combine strong generative model with partial observations?
								<ul class="fragment" data-fragment-index="2">
									<li>Some work in this direction: ReconFusion<br />
									</li>
								</ul>
							</li>
						</ul>
						<img class="fragment" data-fragment-index="2" style="left: 0px; top: 50px; position: relative;" height="400px" data-src="assets/reconfusion.png" />
					</section>
					<section data-auto-animate>
						<h1>Future</h1>
						<h2>Learning vs Optimization</h2>
						<ul>
							<li>Optimization-based approaches inherently limited to what is captured by sensors
								<ul>
									<li>Sufficient for elaborate sensor setups and human operator</li>
									<li>However, far from human-level spatial understanding (infilling / etc.)</li>
								</ul>
							</li>
							<li>Learning approaches might infill from partial observations</li>
							<li>How to combine strong generative model with partial observations?
								<ul>
									<li>Some work in this direction: ReconFusion<br />
									</li>
								</ul>
							</li>
						</ul>
						<img style="left: 0px; top: 50px; position: relative;" height="400px" data-src="assets/reconfusion.png" />
						<video data-autoplay loop style="left: 100px; top: 50px; position: relative;" height="400px" data-src="assets/reconfusion_example.mp4"></video>
					</section>
				</section>
				<section data-auto-animate>
					<section data-auto-animate>
						<h1>Future</h1>
						<h2>Gaussian Splatting</h2>
						<ul>
							<li>Represent scene by a set of 3D Gaussians instead of differentiable function</li>
							<li class="fragment" data-fragment-index="2">Each Gaussian has
								<ul>
									<li>Shape: position + covariance</li>
									<li>Opacity</li>
									<li>Coefficients of spherical harmonics ($\to$ view-dependent color)</li>
								</ul>
							</li>
							<li class="fragment" data-fragment-index="3">
								Rasterization instead of volume rendering <br />
								$\to$ Real-time (>100 FPS), high-resolution rendering
							</li>
						</ul>
					</section>
					<section data-auto-animate>
						<h1>Future</h1>
						<h2>Gaussian Splatting</h2>
						<ul>
							<li>Represent scene by a set of 3D Gaussians instead of differentiable function</li>
							<li>Each Gaussian has
								<ul>
									<li>Shape: position + covariance</li>
									<li>Opacity</li>
									<li>Coefficients of spherical harmonics ($\to$ view-dependent color)</li>
								</ul>
							</li>
							<li>
								Rasterization instead of volume rendering <br />
								$\to$ Real-time (>100 FPS), high-resolution rendering
							</li>
						</ul>
						<video data-autoplay loop style="left: 0px; top: 50px; position: relative;" height="400px" data-src="assets/gs_stump.mp4"></video>
						<video data-autoplay loop style="left: 0px; top: 50px; position: relative;" height="400px" data-src="assets/gs_bicycle.mp4"></video>
						<!-- https://lumalabs.ai/featured -->
					</section>
				</section>
				<section data-auto-animate>
					<h1>Future</h1>
					<h2>Object-based / Interactive Map Decomposition</h2>
					<ul>
						<li>Scenes learned in this way are not interactive
							<ul>
								<li>Objects are not separated; and note complete</li>
								<li>Reflections are baked in</li>
								<li>Shadows are baked in</li>
							</ul>
							<li>Some early work in this direction: LERF, GARField
								<br />
								<video data-autoplay loop style="left: 0px; top: 50px; position: relative;" height="500px" data-src="assets/lerf.mp4"></video>
								<video data-autoplay loop style="left: 50px; top: 50px; position: relative;" height="500px" data-src="assets/garfield_su.mp4"></video>
							</li>
					</ul>
				</section>

			</div>
		</div>

		<script src="dist/reveal.js"></script>
		<script src="plugin/markdown/markdown.js"></script>
		<script src="plugin/highlight/highlight.js"></script>
		<script src="plugin/math/math.js"></script>
		<script>
			// More info about initialization & config:
			// - https://revealjs.com/initialization/
			// - https://revealjs.com/config/
			Reveal.initialize({
							  width: 1920,
							  height: 1080,
							  margin: 0.1,
							  minScale: 0.1,
							  maxScale: 3.0,
							  hash: true,
							  controls: false,
							  progress: true,
							  slideNumber: true,
							  center: false,
							  transition: 'fade',
							  transitionSpeed: 'fast',
							  includeNodes: false,
							  autoPlayMedia: true,
							  navigationMode: 'linear',
							  autoAnimateDuration: 0.3,
							  autoAnimateEasing: 'ease',

							  // Learn about plugins: https://revealjs.com/plugins/
							  plugins: [ RevealMarkdown, RevealHighlight, RevealMath.KaTeX ]
			});
		</script>
	</body>
</html>
