<!doctype html>
<html lang="en">
	<head>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

		<title>Neural Scene Representations</title>

		<link rel="stylesheet" href="dist/reset.css">
		<link rel="stylesheet" href="dist/reveal.css">
		<link rel="stylesheet" href="dist/theme/kth.css">

		<!-- Theme used for syntax highlighted code -->
		<link rel="stylesheet" href="plugin/highlight/monokai.css">
	</head>
	<body>
		<div class="reveal">
			<div class="slides">
				<section class="kthtitle" data-auto-animate>
					<h1>Neural Scene Representations for SLAM</h1>
					<h2>Leonard Bruns</h2>
				</section>
				<section data-auto-animate>
					<h1>Overview</h1>
					<h2></h2>
					<ul>
						<li>Introduction: neural fields, NeRFs, and beyond</li> <!-- 10 minutes -->
						<br />
						<li>Neural SLAM: SLAM with neural scene representations</li> <!-- 10 minutes -->
						<br />
						<li>Neural graph mapping: efficient loop closure in neural SLAM</li>  <!-- 10 minutes -->
						<br />
						<li>Outlook: open research problems</li> <!-- 10 minutes -->
					</ul>
				</section>
				<section class="kthtitle-lhs" data-auto-animate>
					<h1>Introduction</h1>
					<h2>Neural Fields, NeRFs, and beyond</h2>
				</section>
				<section data-auto-animate>
					<h4>Introduction</h4>
					<h1>Shape and Scene Representations</h1>
					<h2></h2>
					<ul>
						<li>Various shape / scene representations exist</li>
						<li class="fragment" data-fragment-index="2">Surface representations
							<ul>
								<li class="fragment" data-fragment-index="2">Store surface position and quantities on surface</li>
								<li class="fragment" data-fragment-index="2">Examples
									<ul>
										<li class="fragment" data-fragment-index="2">Meshes</li>
										<li class="fragment" data-fragment-index="3">Point clouds</li>
									</ul>
								</li>
							</ul>
						</li>
						<li class="fragment" data-fragment-index="4">Volumetric representations
							<ul>
								<li class="fragment" data-fragment-index="4">Store quantities in volume</li>
								<li class="fragment" data-fragment-index="4">Examples
									<ul>
										<li class="fragment" data-fragment-index="4">Voxel grid (dense, octrees, voxel hashing)</li>
										<li class="fragment" data-fragment-index="5">Neural fields</li>
										<li class="fragment" data-fragment-index="6">Set of Gaussians</li>
									</ul>
								</li>
							</ul>
						</li>
					</ul>

					<img class="fragment fade-in-then-out" data-fragment-index="2" style="position: absolute; top: 400px; left: 1300px" src="assets/mesh.png" width="400px">
					<img class="fragment fade-in-then-out" data-fragment-index="3" style="position: absolute; top: 400px; left: 1300px" src="assets/pointcloud.png" width="400px">
					<img class="fragment fade-in-then-out" data-fragment-index="4" style="position: absolute; top: 400px; left: 1300px" src="assets/octree.jpg" width="400px">
					<img class="fragment fade-in-then-out" data-fragment-index="5" style="position: absolute; top: 400px; left: 1100px" src="assets/deepsdf.png" width="600px">
					<div class="fragment fade-in-then-out" data-fragment-index="2" style="position: absolute; bottom: 0px; left: 0px; text-align: left; font-size: 0.6em; color: gray">Image source: Robust Visibility Surface Determination in Object Space via Plücker Coordinates, Rossi et al., 2021</div>
					<div class="fragment fade-in-then-out" data-fragment-index="3" style="position: absolute; bottom: 0px; left: 0px; text-align: left; font-size: 0.6em; color: gray">Image source: Analyzing the Squared Distance-to-Measure Gradient Flow System with k-Order Voronoi Diagrams, O'Neil and Wanner, 2019</div>
					<div class="fragment fade-in-then-out" data-fragment-index="4" style="position: absolute; bottom: 0px; left: 0px; text-align: left; font-size: 0.6em; color: gray">Image source: GPU Gems 2, Chapter 37, Octree Textures on the GPU, Lefebvre et al., 2005</div>
					<div class="fragment fade-in-then-out" data-fragment-index="5" style="position: absolute; bottom: 0px; left: 0px; text-align: left; font-size: 0.6em; color: gray">Image source: DeepSDF: Learning Continuous Signed Distance Functions for Shape Representation, Park et al., 2019</div>
					<!--Octree source https://developer.download.nvidia.com/books/gpugems2/37_octree_03.jpg -->
				</section>
				<section data-auto-animate>
					<h4>Introduction</h4>
					<h1>Neural Fields</h1>
					<h2></h2>
					<ul>
						<li>Core idea: represent 3D shape by a simple neural network
							\[
							\begin{aligned}
							f_\theta: \mathbb{R}^{3} &\to \mathbb{R}^3\times\mathbb{R}\\
							\boldsymbol{x}&\mapsto (\boldsymbol{c}, \rho)
							\end{aligned}
							\]
						</li>
						<li class="fragment" data-fragment-index="2">Map 3D point $\boldsymbol{x}$ to color $\boldsymbol{c}$ and quantity $\rho$ (e.g., SDF, occupancy, etc.)</li>
						<li class="fragment" data-fragment-index="3">Properties
							<ul>
								<li>Continuous</li>
								<li>Differentiable</li>
								<li>Adaptive $\to$ potentially memory efficient</li>
							</ul>
						</li>
					</ul>
					<div style="position: absolute; top: 200px; left: 1300px">
						<img src="assets/staircase.gif" width="500px">
						<br />
						<img src="assets/staircase_sdf.gif" width="500px">
					</div>
					<!-- <img style="position: absolute; top: 150px; right: 20px;" src="assets/staircase.gif" width="25%"> -->
				</section>
				<section data-auto-animate>
					<h4>Introduction</h4>
					<h1>Neural Scene Representations</h1>
					<h2>Early Works</h2>
					<ul>
						<li>DeepSDF [1], occupancy network [2], ...</li>
						<li>Trained using direct 3D supervision (i.e., from mesh collections)</li>
						<li>Demonstrate shape interpolation by adding additional latent input</li>
						\[
						\begin{aligned}
						f_\theta: \mathbb{R}^{3} \times \mathbb{R}^N &\to \mathbb{R}\\
						(\boldsymbol{x}, \boldsymbol{z})&\mapsto \rho
						\end{aligned}
						\]
					</ul>
					<br />
					<img style="margin-left:200px" src="assets/deepsdf.png" height="300px">
					<video data-autoplay loop style="margin-left: 200px" height="300px" data-src="assets/deepsdf_interpolation.webm"></video>
					<div style="position: absolute; bottom: 0px; left: 0px; text-align: left; font-size: 0.6em; color: gray">[1] DeepSDF: Learning Continuous Signed Distance Functions for Shape Representation, Park et al., 2019<br />[2] Occupancy Networks: Learning 3D Reconstruction in Function Space, Mescheder et al., 2019<br />Video source: https://www.youtube.com/watch?v=C_XNdGGs6qM </div>
				</section>
				<section data-auto-animate>
					<h4>Introduction</h4>
					<h1>Neural Scene Representations</h1>
					<h2>Neural Radiance Field (NeRF)</h2>
					<ul>
						<li>Train neural field using indirect 2D supervision</li>
						<li>Camera poses fixed (e.g., from structure-from-motion)</li>
						<li>Add 2D view direction as additional input</li>
						\[
						\begin{aligned}
						f_\theta: \mathbb{R}^{3} \times \mathbb{R}^2 &\to \mathbb{R}^3\times\mathbb{R}\\
						(\boldsymbol{x}, \boldsymbol{d})&\mapsto (\boldsymbol{c}, \sigma)
						\end{aligned}
						\]
					</ul>
					<br />
					<br />
					<img style="margin-left:200px" src="assets/nerf_overview.png" width="75%">
					<div style="position: absolute; bottom: 0px; left: 0px; text-align: left; font-size: 0.6em; color: gray">NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis, Mildenhall et al., 2020</div>
				</section>
				<section data-auto-animate>
					<section data-auto-animate>
						<h4>Introduction</h4>
						<h1>Neural Scene Representations</h1>
						<h2>Neural Radiance Field (NeRF)</h2>
						<ul>
							<li>NeRF Examples<br />
								<video loop style="" height="350px">
									<source data-autoplay data-src="assets/nerf_1.mp4">
								</video>
								<video loop style="" height="350px">
									<source data-autoplay data-src="assets/nerf_2.mp4">
								</video>
								<video loop style="" height="350px">
									<source data-autoplay data-src="assets/nerf_3.mp4">
								</video>
								<span class="fragment fade-in">
									<video data-autoplay loop height="350px" data-src="assets/nerf_depth.mp4" />
								</span>
							</li>
						</ul>
					</section>
					<section data-auto-animate>
						<h4>Introduction</h4>
						<h1>Neural Scene Representations</h1>
						<h2>Neural Radiance Field (NeRF)</h2>
						<ul>
							<li>NeRF Examples<br />
								<video loop style="" height="250px">
									<source data-autoplay data-src="assets/nerf_1.mp4">
								</video>
								<video loop style="" height="250px">
									<source data-autoplay data-src="assets/nerf_2.mp4">
								</video>
								<video loop style="" height="250px">
									<source data-autoplay data-src="assets/nerf_3.mp4">
								</video>
								<span>
									<video data-autoplay loop height="250px" data-src="assets/nerf_depth.mp4" />
								</span>
							</li>
							<li>Limitations
								<ul>
									<li>Mainly limited to small forward-facing scenes</li>
									<li>Isolevel for surfaces not well-defined</li>
									<li>Optimization time (~hours)</li>
									<li>Rendering time (~seconds for single 1080p frame)</li>
								</ul>
							</li>
						</ul>
					</section>
					<div style="position: absolute; bottom: 0px; left: 0px; text-align: left; font-size: 0.6em; color: gray">NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis, Mildenhall et al., 2020</div>
				</section>
				<section data-auto-animate>
					<section data-auto-animate>
						<h4>Introduction</h4>
						<h1>Neural Scene Representations</h1>
						<h2>Follow-up Works</h2>
						<ul>
							<li>Surfaces: NeuS [1] <br />
								<img src="assets/neus.png" height="600px">
							</li>
						</ul>
						<div data-id="footnote" style="position: absolute; bottom: 0px; left: 0px; text-align: left; font-size: 0.6em; color: gray">
							<div data-id="footnote-1">[1] NeuS: Learning Neural Implicit Surfaces by Volume Rendering for Multi-view Reconstruction, Wang et al., 2021</div>
							<div style="opacity: 0%" data-id="footnote-2">[2] Instant Neural Graphics Primitives with a Multiresolution Hash Encoding, Müller et al., 2022</div>
							<div style="opacity: 0%" data-id="footnote-3">[3] Zip-NeRF: Anti-Aliased Grid-Based Neural Radiance Fields, Barron et al., 2023</div>
						</div>
					</section>
					<section data-auto-animate>
						<h4>Introduction</h4>
						<h1>Neural Scene Representations</h1>
						<h2>Follow-up Works</h2>
						<ul>
							<li>Surfaces: NeuS [1] <br />
								<img src="assets/neus.png" height="150px">
							</li>
							<li>Speed: Instant Neural Graphics Primitives [2]<br />
								<video loop data-autoplay height="400px" data-src="assets/ingp.mp4" ></video>
							</li>
						</ul>
						<div data-id="footnote" style="position: absolute; bottom: 0px; left: 0px; text-align: left; font-size: 0.6em; color: gray">
							<div data-id="footnote-1">[1] NeuS: Learning Neural Implicit Surfaces by Volume Rendering for Multi-view Reconstruction, Wang et al., 2021</div>
							<div data-id="footnote-2">[2] Instant Neural Graphics Primitives with a Multiresolution Hash Encoding, Müller et al., 2022</div>
							<div style="opacity: 0%" data-id="footnote-3">[3] Zip-NeRF: Anti-Aliased Grid-Based Neural Radiance Fields, Barron et al., 2023</div>
						</div>
					</section>
					<section data-auto-animate>
						<h4>Introduction</h4>
						<h1>Neural Scene Representations</h1>
						<h2>Follow-up Works</h2>
						<ul>
							<li>Surfaces: NeuS [1] <br />
								<img src="assets/neus.png" height="150px">
							</li>
							<li>Speed: Instant Neural Graphics Primitives [2]<br />
								<video loop data-autoplay height="150px" data-src="assets/ingp.mp4" />
							</li>
							<li>Quality: Zip-NeRF [3]<br />
							</li>
							<video style="left: 900px; bottom: 570px;position: relative;" loop data-autoplay height="700px" data-src="assets/zipnerf.mp4"></video>
						</ul>
						<div data-id="footnote" style="position: absolute; bottom: 0px; left: 0px; text-align: left; font-size: 0.6em; color: gray">
							<div data-id="footnote-1">[1] NeuS: Learning Neural Implicit Surfaces by Volume Rendering for Multi-view Reconstruction, Wang et al., 2021</div>
							<div data-id="footnote-2">[2] Instant Neural Graphics Primitives with a Multiresolution Hash Encoding, Müller et al., 2022</div>
							<div data-id="footnote-3">[3] Zip-NeRF: Anti-Aliased Grid-Based Neural Radiance Fields, Barron et al., 2023</div>
						</div>
					</section>
					<section data-auto-animate>
						<h4>Introduction</h4>
						<h1>Neural Scene Representations</h1>
						<h2>Follow-up Works</h2>
						<ul>
							<li>Surfaces: NeuS [1] <br />
								<img src="assets/neus.png" height="150px">
							</li>
							<li>Speed: Instant Neural Graphics Primitives [2]<br />
								<video loop data-autoplay height="150px" data-src="assets/ingp.mp4" />
							</li>
							<li>Quality: Zip-NeRF [3]<br />
							</li>
							<video style="left: 0; top: 0;position: relative;" loop data-autoplay height="150px" data-src="assets/zipnerf.mp4"></video>
						</ul>
						<div data-id="footnote" style="position: absolute; bottom: 0px; left: 0px; text-align: left; font-size: 0.6em; color: gray">
							<div data-id="footnote-1">[1] NeuS: Learning Neural Implicit Surfaces by Volume Rendering for Multi-view Reconstruction, Wang et al., 2021</div>
							<div data-id="footnote-2">[2] Instant Neural Graphics Primitives with a Multiresolution Hash Encoding, Müller et al., 2022</div>
							<div data-id="footnote-3">[3] Zip-NeRF: Anti-Aliased Grid-Based Neural Radiance Fields, Barron et al., 2023</div>
						</div>
					</section>
				</section>
				<section class="kthtitle-lhs" data-auto-animate>
					<h1>Neural SLAM</h1>
					<h2>SLAM with Neural Scene Representations</h2>
				</section>
				<section data-auto-animate>
					<h4>Neural SLAM</h4>
					<h1>Dense Visual SLAM</h1>
					<h2></h2>
					<ul>
						<li>Visual Simultaneous Localization and Mapping (SLAM)
							<ul>
								<li>Estimate camera trajectory of a moving camera from its image stream</li>
								<li>Sparse keypoint-based maps</li>
							</ul>
						</li>
						<li  class="fragment">Dense visual SLAM
							<ul>
								<li>Estimate dense 3D scene representation during localization (e.g., mesh)</li>
								<li>RGB-D data</li>
								<li>Volumetric scene representation
									<ul>
										<li>Classic approaches use voxel grids to represent 3D scene</li>
									</ul>
								</li>
							</ul>
							<video data-autoplay loop style="left: 0px; top: 50px; position: relative;" height="300px" data-src="assets/slam_input.mp4"></video>
							<video data-autoplay loop style="left: 100px; top: 50px; position: relative;" height="400px" data-src="assets/slam_output.mp4"></video>
						</li>
					</ul>
				</section>
				<section data-auto-animate>
					<h4>Neural SLAM</h4>
					<h1>iMAP</h1>
					<h2></h2>
					<ul>
						<li>First work demonstrating SLAM using neural field as scene representation</li>
						<li>Alternates between frame-to-map alignment and map refinement</li>
						<li  class="fragment" data-fragment-index="1">Compared to NeRF
							<ul>
								<li>Removed view-direction dependency</li>
								<li>Smaller neural network</li>
								<li>Depth supervision</li>
							</ul>
						</li>
						<img  class="fragment" data-fragment-index="2" src="assets/imap.gif" height="400px">

						<li  class="fragment" data-fragment-index="3" >Limitation: continuous reoptimization of previous areas required to prevent forgetting</li>
					</ul>
					<div data-id="footnote" style="position: absolute; bottom: 0px; left: 0px; text-align: left; font-size: 0.6em; color: gray">iMAP: Implicit Mapping and Positioning in Real-Time, Sucar et al., 2021</div>
				</section>
				<section data-auto-animate>
					<section data-auto-animate>
						<h4>Neural SLAM</h4>
						<h1>NICE-SLAM</h1>
						<h2></h2>
						<ul>
							<li>Replaces single MLP by combination of feature grid and frozen, pretrained decoder
								<br />$\to$ No more forgetting, better scalability
							</li>
						</ul>
						<img src="assets/niceslam.png" height="600px">
					</section>
					<section data-auto-animate>
						<h4>Neural SLAM</h4>
						<h1>NICE-SLAM</h1>
						<h2></h2>
						<ul>
							<li>Replaces single MLP by combination of feature grid and frozen, pretrained decoder
								<br />$\to$ No more forgetting, better scalability
							</li>
						</ul>
						<video data-autoplay loop style="left: 0px; top: 0px;position: relative;" height="750px" data-src="assets/niceslam.mp4" />
					</section>
					<div style="position: absolute; bottom: 0px; left: 0px; text-align: left; font-size: 0.6em; color: gray">NICE-SLAM: Neural Implicit Scalable Encoding for SLAM, Zhu et al., 2022</div>
				</section>
				<section data-auto-animate>
					<section data-auto-animate>
						<h4>Neural SLAM</h4>
						<h1>Follow-up Works</h1>
						<h2></h2>
						<ul style="width: 100%">
							<li>ESLAM [1]: triplane encoding
								<br />
								<img src="assets/eslam.jpg" height="700px">
							</li>
						</ul>
						<div data-id="footnote" style="position: absolute; bottom: 0px; left: 0px; text-align: left; font-size: 0.6em; color: gray">
							<div data-id="footnote-1">[1] ESLAM: Efficient Dense SLAM System Based on Hybrid Representation of Signed Distance Fields, Johari et al., 2023</div>
							<div style="opacity: 0%" data-id="footnote-2">[2] Co-SLAM: Joint Coordinate and Sparse Parametric Encodings for Neural Real-Time SLAM, Wang et al., 2023</div>
							<div style="opacity: 0%" data-id="footnote-3">[3] GO-SLAM: Global Optimization for Consistent 3D Instant Reconstruction, Zhang et al., 2023</div>
						</div>
					</section>
					<section data-auto-animate>
						<h4>Neural SLAM</h4>
						<h1>Follow-up Works</h1>
						<h2></h2>
						<ul style="width: 100%">
							<li>ESLAM [1]: triplane encoding
								<br />
								<img src="assets/eslam.jpg" height="150px">
							</li>
							<li>Co-SLAM [2]: joint hash / coordinate encoding
								<br />
								<img src="assets/coslam.png" height="450px" />
							</li>
						</ul>
						<div data-id="footnote" style="position: absolute; bottom: 0px; left: 0px; text-align: left; font-size: 0.6em; color: gray">
							<div data-id="footnote-1">[1] ESLAM: Efficient Dense SLAM System Based on Hybrid Representation of Signed Distance Fields, Johari et al., 2023</div>
							<div data-id="footnote-2">[2] Co-SLAM: Joint Coordinate and Sparse Parametric Encodings for Neural Real-Time SLAM, Wang et al., 2023</div>
							<div style="opacity: 0%" data-id="footnote-3">[3] GO-SLAM: Global Optimization for Consistent 3D Instant Reconstruction, Zhang et al., 2023</div>
						</div>
					</section>
					<section data-auto-animate>
						<h4>Neural SLAM</h4>
						<h1>Follow-up Works</h1>
						<h2></h2>
						<ul style="width: 100%">
							<li>ESLAM [1]: triplane encoding
								<br />
								<img src="assets/eslam.jpg" height="150px">
							</li>
							<li>Co-SLAM [2]: joint hash / coordinate encoding
								<br />
								<img src="assets/coslam.png" height="150px" />
							</li>
							<li>GO-SLAM [3]: loop closure via reoptimization
								<br />
								<img style="position: relative; left: 900px; top: -270px;"
			 src="assets/goslam.png" height="450" />
							</li>
						</ul>
						<div data-id="footnote" style="position: absolute; bottom: 0px; left: 0px; text-align: left; font-size: 0.6em; color: gray">
							<div data-id="footnote-1">[1] ESLAM: Efficient Dense SLAM System Based on Hybrid Representation of Signed Distance Fields, Johari et al., 2023</div>
							<div data-id="footnote-2">[2] Co-SLAM: Joint Coordinate and Sparse Parametric Encodings for Neural Real-Time SLAM, Wang et al., 2023</div>
							<div data-id="footnote-3">[3] GO-SLAM: Global Optimization for Consistent 3D Instant Reconstruction, Zhang et al., 2023</div>
						</div>
					</section>
					<section data-auto-animate>
						<h4>Neural SLAM</h4>
						<h1>Follow-up Works</h1>
						<h2></h2>
						<ul style="width: 100%">
							<li>ESLAM [1]: triplane encoding
								<br />
								<img src="assets/eslam.jpg" height="150px">
							</li>
							<li>Co-SLAM [2]: joint hash / coordinate encoding
								<br />
								<img src="assets/coslam.png" height="150px" />
							</li>
							<li>GO-SLAM [3]: loop closure via reoptimization
								<br />
								<img style="position: relative; left: 0px; top: 0px;"
			 src="assets/goslam.png" height="150" />
							</li>
						</ul>
						<div data-id="footnote" style="position: absolute; bottom: 0px; left: 0px; text-align: left; font-size: 0.6em; color: gray">
							<div data-id="footnote-1">[1] ESLAM: Efficient Dense SLAM System Based on Hybrid Representation of Signed Distance Fields, Johari et al., 2023</div>
							<div data-id="footnote-2">[2] Co-SLAM: Joint Coordinate and Sparse Parametric Encodings for Neural Real-Time SLAM, Wang et al., 2023</div>
							<div data-id="footnote-3">[3] GO-SLAM: Global Optimization for Consistent 3D Instant Reconstruction, Zhang et al., 2023</div>
						</div>
					</section>
				</section>
				<section data-auto-animate>
					<h4>Neural SLAM</h4>
					<h1>Common Limitation: Monolithic Field</h1>
					<h2></h2>
					<ul>
						<li>All methods use a single monolithic data structure</li>
						<br />
						<li>Expensive to optimize neural fields</li>
						<br />
						<li>Sensor data gets baked into a data structure
							<ul>
								<li>Similar to classic voxel-based data structures</li>
							</ul>
						</li>
						<br />
						<li>Cannot easily deform volumetric scene representation on loop closure
							<ul>
								<li>GO-SLAM has to reintegrate all keyframes that have moved on loop closure<br />
									$\to$ The larger the loop, the more reoptimization is required

								</li>
							</ul>
						</li>
					</ul>
				</section>
				<section class="kthtitle-lhs" data-auto-animate>
					<h1 data-id="header">Neural Graph Mapping</h1>
					<h2>Efficient Loop Closure for Neural SLAM</h2>
				</section>
				<section>
					<h1 data-id="header">Neural Graph Mapping</h1>
					<section>
						<ul>
							<li>Idea: attach lightweight neural fields to pose graph of sparse visual SLAM</li>
							<li>Each field captures scene within a fixed radius $r$</li>
						</ul>
						<img style="position: relative; top: 70px;" src="assets/ngm_anim_0.png" height="700px">
					</section>
					<section data-transition="none">
						<ul>
							<li>Idea: attach lightweight neural fields to pose graph of sparse visual SLAM</li>
							<li>Each field captures scene within a fixed radius $r$</li>
						</ul>
						<img style="position: relative; top: 70px;" src="assets/ngm_anim_1.png" height="700px">
					</section>
					<section data-transition="none">
						<ul>
							<li>Idea: attach lightweight neural fields to pose graph of sparse visual SLAM</li>
							<li>Each field captures scene within a fixed radius $r$</li>
						</ul>
						<img style="position: relative; top: 70px;" src="assets/ngm_anim_2.png" height="700px">
					</section>
					<section data-transition="none">
						<ul>
							<li>Idea: attach lightweight neural fields to pose graph of sparse visual SLAM</li>
							<li>Each field captures scene within a fixed radius $r$</li>
						</ul>
						<img style="position: relative; top: 70px;" src="assets/ngm_anim_3.png" height="700px">
					</section>
					<section data-auto-animate data-transition="none">
						<ul>
							<li>Idea: attach lightweight neural fields to pose graph of sparse visual SLAM</li>
							<li>Each field captures scene within a fixed radius $r$</li>
						</ul>
						<img style="position: relative; top: 70px;" src="assets/ngm_anim_4.png" height="700px">
					</section>
					<section data-auto-animate>
						<ul>
							<li>Idea: attach lightweight neural fields to pose graph of sparse visual SLAM</li>
							<li>Each field captures scene within a fixed radius $r$</li>
						</ul>
						<img style="position: relative; top: 70px;" src="assets/ngm_anim_4.png" height="700px">
						<img style="position: relative; top: 70px; left: 300px;" src="assets/ngm_example.png" height="700px">
					</section>
					<div style="position: absolute; bottom: 0px; left: 0px; text-align: left; font-size: 0.6em; color: gray">Neural Graph Mapping for Scalable Dense SLAM with Loop Closure, Bruns et al., 2024</div>
				</section>
				<section data-auto-animate>
					<h4>Neural Graph Mapping</h4>
					<h1>Example</h1>
					<video style="left: 0; top: 0;position: relative;" loop data-autoplay height="850px" data-src="assets/ngm_scannet.mp4" />
				</section>
				<section data-auto-animate>
					<h4>Neural Graph Mapping</h4>
					<h1>Another Example</h1>
					<video style="left: 0; top: 0;position: relative;" loop data-autoplay height="600px" data-src="assets/ngm_apartment0_cut.mp4" />
				</section>
				<section data-auto-animate>
					<h4>Neural Graph Mapping</h4>
					<h1>Comparison to Monolithic Approaches</h1>
					<img style="left: 0; top: 0;position: relative;" src="assets/ngm_comparison.png" height="800px" />
				</section>
				<section data-auto-animate>
					<h4>Neural Graph Mapping</h4>
					<h1>Outlook</h1>
					<ul>
						<li>Higher memory usage compared to monolithic neural field methods</li>
						<ul>
							<li>How to reduce memory requirements?</li>
						</ul>
						</li>
						<br />
						<li class="fragment" data-fragment-index="1">Neural field-based map not used for SLAM currently
							<ul>
								<li>How to combine sparse visual SLAM and dense visual SLAM?</li>
							</ul>
						</li>
						<br />
						<li class="fragment" data-fragment-index="2">Currently limited to indoor environments with depth data available
							<ul>
								<li>How to extend to outdoor environments?</li>
							</ul>
						</li>
						<br />
						<li class="fragment" data-fragment-index="3">Neural SLAM in general requires lots of resources
							<ul>
								<li>Can we use meta-learning for few-shot mapping?</li>
							</ul>
						</li>
					</ul>
				</section>
				<section class="kthtitle-lhs">
					<h1>Outlook</h1>
					<h2>Open Research Problems</h2>
				</section>
				<section data-auto-animate>
					<section data-auto-animate>
						<h1>Outlook</h1>
						<h2>Learning vs Optimization</h2>
						<ul>
							<li>Optimization-based approaches inherently limited to what is captured by sensors
								<ul>
									<li>Sufficient for elaborate sensor setups and human operator</li>
									<li>However, far from human-level spatial understanding (infilling / etc.)</li>
								</ul>
							</li>
							<li>Learning approaches might infill from partial observations</li>
							<li class="fragment" data-fragment-index="1">How to combine strong generative model with partial observations?
								<ul class="fragment" data-fragment-index="2">
									<li>Some work in this direction: ReconFusion<br />
									</li>
								</ul>
							</li>
						</ul>
						<img class="fragment" data-fragment-index="2" style="left: 0px; top: 50px; position: relative;" height="400px" data-src="assets/reconfusion.png" />
						<span class="fragment" data-fragment-index="3">
							<video  data-autoplay loop style="left: 100px; top: 50px; position: relative;" height="400px" data-src="assets/reconfusion_example.mp4"></video>
						</span>
						<div class="fragment" data-fragment-index="2" style="position: absolute; bottom: 0px; left: 0px; text-align: left; font-size: 0.6em; color: gray">ReconFusion: 3D Reconstruction with Diffusion Priors, Wu et al., 2023</div>
					</section>
				</section>
				<section data-auto-animate>
					<section data-auto-animate>
						<h1>Outlook</h1>
						<h2>Gaussian Splatting</h2>
						<ul>
							<li>Represent scene by a set of 3D Gaussians instead of differentiable function</li>
							<li class="fragment" data-fragment-index="2">Each Gaussian has
								<ul>
									<li>Shape: position + covariance</li>
									<li>Opacity</li>
									<li>Coefficients of spherical harmonics ($\to$ view-dependent color)</li>
								</ul>
							</li>
							<li class="fragment" data-fragment-index="3">
								Rasterization instead of volume rendering <br />
								$\to$ Real-time (>100 FPS), high-resolution rendering
							</li>
						</ul>
						<div data-id="footnote" style="position: absolute; bottom: 0px; left: 0px; text-align: left; font-size: 0.6em; color: gray">3D Gaussian Splatting for Real-Time Radiance Field Rendering, Kerbl et al., 2023<br />
							<a href="https://lumalabs.ai/featured">https://lumalabs.ai/featured</a>
						</div>
					</section>
					<section data-auto-animate>
						<h1>Outlook</h1>
						<h2>Gaussian Splatting</h2>
						<ul>
							<li>Represent scene by a set of 3D Gaussians instead of differentiable function</li>
							<li>Each Gaussian has
								<ul>
									<li>Shape: position + covariance</li>
									<li>Opacity</li>
									<li>Coefficients of spherical harmonics ($\to$ view-dependent color)</li>
								</ul>
							</li>
							<li>
								Rasterization instead of volume rendering <br />
								$\to$ Real-time (>100 FPS), high-resolution rendering
							</li>
						</ul>
						<video data-autoplay loop style="left: 0px; top: 50px; position: relative;" height="400px" data-src="assets/gs_stump.mp4"></video>
						<video data-autoplay loop style="left: 0px; top: 50px; position: relative;" height="400px" data-src="assets/gs_bicycle.mp4"></video>
						<!-- https://lumalabs.ai/featured -->
						<div data-id="footnote" style="position: absolute; bottom: 0px; left: 0px; text-align: left; font-size: 0.6em; color: gray">3D Gaussian Splatting for Real-Time Radiance Field Rendering, Kerbl et al., 2023<br />
							<a href="https://lumalabs.ai/featured">https://lumalabs.ai/featured</a>
						</div>
					</section>
				</section>
				<section data-auto-animate>
					<h1>Outlook</h1>
					<h2>Object-based / Interactive Map Decomposition</h2>
					<ul>
						<li>Scenes learned in this way are not interactive
							<ul>
								<li>Objects are not separated; and not complete</li>
								<li>Reflections are baked in</li>
								<li>Shadows are baked in</li>
							</ul>
							<li class="fragment" data-fragment-index="2">
								Some early work in this direction: LERF [1], GARField [2]
								<br />
								<video data-autoplay loop style="left: 0px; top: 50px; position: relative;" height="500px" data-src="assets/lerf.mp4"></video>
								<video data-autoplay loop style="left: 50px; top: 50px; position: relative;" height="500px" data-src="assets/garfield_su.mp4"></video>
							</li>
					</ul>
					<div class="fragment" data-fragment-index="2" style="position: absolute; bottom: 0px; left: 0px; text-align: left; font-size: 0.6em; color: gray">[1] LERF: Language Embedded Radiance Fields, Kerr et al., 2023<br />[2] GARField: Group Anything with Radiance Fields, Kim et al., 2024</div>
				</section>
				<section class="kthlogo" data-auto-animate>
				</section>
			</div>
		</div>

		<script src="dist/reveal.js"></script>
		<script src="plugin/markdown/markdown.js"></script>
		<script src="plugin/highlight/highlight.js"></script>
		<script src="plugin/math/math.js"></script>
		<script>
			// More info about initialization & config:
			// - https://revealjs.com/initialization/
			// - https://revealjs.com/config/
			Reveal.initialize({
							  width: 1920,
							  height: 1080,
							  margin: 0.1,
							  minScale: 0.1,
							  maxScale: 3.0,
							  hash: true,
							  controls: false,
							  progress: true,
							  slideNumber: true,
							  center: false,
							  transition: 'fade',
							  transitionSpeed: 'fast',
							  includeNodes: false,
							  autoPlayMedia: true,
							  navigationMode: 'linear',
							  autoAnimateDuration: 0.3,
							  autoAnimateEasing: 'ease',

							  // Learn about plugins: https://revealjs.com/plugins/
							  plugins: [ RevealMarkdown, RevealHighlight, RevealMath.KaTeX ]
			});
		</script>
	</body>
</html>
